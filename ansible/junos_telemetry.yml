---
# Initialize run timestamp before any processing
- name: Initialize Run Timestamp
  hosts: localhost
  connection: local
  gather_facts: no
  tasks:
    - name: Set current UTC date and hour for this playbook run
      set_fact:
        run_timestamp: "{{ lookup('pipe', 'date -u +%s') | int }}"
        run_date: "{{ lookup('pipe', 'date -u +%Y-%m-%d') }}"
        run_hour: "{{ lookup('pipe', 'date -u +%H') }}"
        partition_dir: "dt={{ lookup('pipe', 'date -u +%Y-%m-%d') }}/hr={{ lookup('pipe', 'date -u +%H') }}"
        inventory_group: "{{ groups.keys() | difference(['all', 'ungrouped']) | first }}"
        cacheable: yes
    
    - name: Display run timestamp
      debug:
        msg: "Playbook run partition: {{ partition_dir }}, Group: {{ inventory_group }}"

- name: Collect Junos Telemetry and Export to Prometheus
  hosts: all
  connection: local
  gather_facts: no
  
  vars:
    netconf_port: 830
    output_dir: "/tmp/semaphore/output/{{ hostvars['localhost']['inventory_group'] }}_{{ hostvars['localhost']['run_timestamp'] }}"
    raw_ml_data_dir: "/tmp/semaphore/raw_ml_data"
    prometheus_pushgateway: "{{ lookup('env', 'PROMETHEUS_PUSHGATEWAY') | default('http://10.221.80.101:9091', true) }}"
    rpc_config_file: "rpc_commands.yml"
    # Optional: Comma-separated list of interfaces to monitor (e.g., "et-0/0/32,et-0/0/33")
    # If not set or empty, all interfaces will be monitored
    interface_filter: ""
  
  tasks:
    - name: Ensure output directory exists for this run
      file:
        path: "{{ output_dir }}"
        state: directory
        mode: '0777'
      delegate_to: localhost
      run_once: true
      changed_when: false
    
    - name: Load RPC commands configuration
      include_vars:
        file: "{{ rpc_config_file }}"
        name: rpc_config
    
    - name: Execute RPC commands on Junos device
      junipernetworks.junos.junos_rpc:
        rpc: "{{ item.rpc }}"
        args: "{{ item.rpc_args | default({}) }}"
        output: xml
      register: rpc_results
      loop: "{{ rpc_config.rpc_commands }}"
      loop_control:
        label: "{{ item.name }}"
    
    - name: Debug RPC output structure
      debug:
        msg: 
          - "Output type: {{ item.output | type_debug }}"
          - "Output length: {{ item.output | length if item.output is sequence else 'N/A' }}"
          - "Has output_lines: {{ 'output_lines' in item }}"
      loop: "{{ rpc_results.results }}"
      loop_control:
        label: "{{ item.item.name }}"
      when: false  # Set to true to enable debugging
    
    - name: Save raw RPC outputs
      copy:
        content: "{{ item.output | join('') if item.output is sequence else item.output }}"
        dest: "{{ output_dir }}/{{ inventory_hostname }}_{{ item.item.name }}_raw.xml"
      loop: "{{ rpc_results.results }}"
      loop_control:
        label: "{{ item.item.name }}"
    
    - name: Determine interface filter for this device
      set_fact:
        device_interface_filter: "{{ hostvars[inventory_hostname].interface_filter | default(interface_filter) }}"
    
    - name: Parse RPC outputs and convert to JSON format
      script: >
        parsers/{{ item.item.parser }}.py
        --input "{{ output_dir }}/{{ inventory_hostname }}_{{ item.item.name }}_raw.xml"
        --output "{{ output_dir }}/{{ inventory_hostname }}_{{ item.item.name }}_metrics.json"
        --device "{{ inventory_hostname }}"
        {% if device_interface_filter and (item.item.name == 'optics_diagnostics' or item.item.name == 'interface_statistics') %}--interfaces "{{ device_interface_filter }}"{% endif %}
        {% if item.item.name != 'interface_statistics' %}--format json{% endif %}
      args:
        executable: python3
        chdir: "{{ playbook_dir }}"
      environment:
        PYTHONPATH: "{{ playbook_dir }}"
      loop: "{{ rpc_results.results }}"
      loop_control:
        label: "{{ item.item.name }}"
      register: parse_results
    
    - name: Display parsing results
      debug:
        msg: "Parsed {{ item.item.item.name }}: {{ item.stdout_lines }}"
      loop: "{{ parse_results.results }}"
      loop_control:
        label: "{{ item.item.item.name }}"
      when: false  # Enable for debugging
    
    - name: Collect PIC details for detailed transceiver information
      script: >
        scripts/collect_pic_details.py
        --host "{{ inventory_hostname }}"
        --username "{{ ansible_user }}"
        --password "{{ ansible_password }}"
        --port "{{ ansible_port | default(830) }}"
        --chassis-xml "{{ output_dir }}/{{ inventory_hostname }}_chassis_inventory_raw.xml"
        --output "{{ output_dir }}/{{ inventory_hostname }}_pic_detail_metrics.json"
        {% if hostvars[inventory_hostname].hardware_model is defined %}--platform "{{ hostvars[inventory_hostname].hardware_model }}"{% endif %}
      args:
        executable: python3
        chdir: "{{ playbook_dir }}"
      environment:
        PYTHONPATH: "{{ playbook_dir }}"
      when: "'chassis_inventory' in rpc_config.rpc_commands | map(attribute='name')"
      register: pic_detail_result
      ignore_errors: yes
    
    - name: Display PIC detail collection result
      debug:
        msg: "{{ pic_detail_result.stdout_lines }}"
      when: false  # Enable for debugging
    
    - name: Merge metadata into optics diagnostics
      script: >
        parsers/juniper/merge_metadata.py
        --system-info "{{ output_dir }}/{{ inventory_hostname }}_system_information_metrics.json"
        --chassis-inventory "{{ output_dir }}/{{ inventory_hostname }}_chassis_inventory_metrics.json"
        {% if pic_detail_result is defined and pic_detail_result.rc == 0 %}--pic-detail "{{ output_dir }}/{{ inventory_hostname }}_pic_detail_metrics.json"{% endif %}
        --optics-metrics "{{ output_dir }}/{{ inventory_hostname }}_optics_diagnostics_metrics.json"
        --output "{{ output_dir }}/{{ inventory_hostname }}_optics_diagnostics_metrics.json"
      args:
        executable: python3
        chdir: "{{ playbook_dir }}"
      environment:
        PYTHONPATH: "{{ playbook_dir }}"
      when: "'optics_diagnostics' in rpc_config.rpc_commands | map(attribute='name')"
      register: merge_result
      ignore_errors: yes
    
    - name: Display merge result
      debug:
        msg: "{{ merge_result.stdout_lines }}"
      when: false  # Enable for debugging
    
    - name: Push metrics to Prometheus Pushgateway
      script: >
        scripts/push_to_prometheus.py
        --pushgateway "{{ prometheus_pushgateway }}"
        --job "junos_telemetry"
        --instance "{{ inventory_hostname }}"
        --metrics-file "{{ output_dir }}/{{ inventory_hostname }}_{{ item.item.name }}_metrics.json"
        --format json
      args:
        executable: python3
      loop: "{{ rpc_results.results }}"
      loop_control:
        label: "{{ item.item.name }}"
      when: prometheus_pushgateway is defined

# Aggregate all devices into hourly Parquet files
- name: Write Hourly Parquet Files
  hosts: localhost
  connection: local
  gather_facts: no
  
  vars:
    # output_dir: "{{ '../output' if playbook_dir != '/ansible' else '/output' }}"
    # raw_ml_data_dir: "{{ '/raw_ml_data' if playbook_dir == '/ansible' else '../raw_ml_data' }}"
    output_dir: "/tmp/semaphore/output/{{ hostvars['localhost']['inventory_group'] }}_{{ hostvars['localhost']['run_timestamp'] }}"
    raw_ml_data_dir: "/tmp/semaphore/raw_ml_data"
  tasks:
    - name: Get runner name from container environment
      shell: echo $SEMAPHORE_RUNNER_NAME
      register: runner_name_result
    
    - name: Get cluster name from container environment
      shell: echo $CLUSTER_NAME
      register: cluster_name_result
    
    - name: Set runner name variable
      set_fact:
        runner_name: "{{ runner_name_result.stdout }}"
        cluster_name: "{{ cluster_name_result.stdout }}"
    
    - name: Fail if runner name is not set
      fail:
        msg: "SEMAPHORE_RUNNER_NAME environment variable must be set"
      when: runner_name == ''
    
    - name: Fail if cluster name is not set
      fail:
        msg: "CLUSTER_NAME environment variable must be set"
      when: cluster_name == ''
    
    - name: Write aggregated hourly Parquet files (interface_dom, lane_dom, interface_counters)
      script: >
        scripts/write_hourly_parquet.py
        --metrics-dir "{{ output_dir }}"
        --base-dir "{{ raw_ml_data_dir }}"
        --cluster-name "{{ cluster_name }}"
        --runner-name "{{ runner_name }}"
        --partition-dir "{{ hostvars['localhost']['partition_dir'] }}"
        --run-timestamp "{{ hostvars['localhost']['run_timestamp'] }}"
        --compression snappy
      args:
        executable: python3
        chdir: "{{ playbook_dir }}"
      environment:
        PYTHONPATH: "{{ playbook_dir }}"
      register: hourly_parquet_result
    
    - name: Fail if hourly Parquet write failed
      fail:
        msg: "Hourly Parquet file creation failed with return code {{ hourly_parquet_result.rc }}"
      when: hourly_parquet_result.rc != 0
    
    - name: Display hourly Parquet results
      debug:
        msg: "{{ hourly_parquet_result.stdout_lines }}"
      when: hourly_parquet_result.stdout_lines is defined

    - name: Sync current hour data to S3 datalake
      community.aws.s3_sync:
        bucket: amzn-ds-s3-rrd
        file_root: "{{ raw_ml_data_dir }}/{{ hostvars['localhost']['partition_dir'] }}"
        key_prefix: "datalake/{{ hostvars['localhost']['partition_dir'] }}/"
        region: us-east-1
      environment:
        AWS_ACCESS_KEY_ID: "{{ aws_access_key_id }}"
        AWS_SECRET_ACCESS_KEY: "{{ aws_secret_access_key }}"
        AWS_SESSION_TOKEN: "{{ aws_session_token | default('') }}"
      register: s3_sync_result
      when: aws_access_key_id is defined and aws_secret_access_key is defined

    - name: Display S3 sync result
      debug:
        msg: "{{ s3_sync_result }}"
      when: false  # Enable for debugging
